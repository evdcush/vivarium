{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical session 4: Modulating internal states and Sensing other agent's attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last practical session, we saw how to run multiple behaviors in parallel on multiple agents. In this section we will see more advanced methods for combining behaviors by modulating their activations according to internal states of the agent and by allowing them to sense the attributes of others (e.g. their color or their size). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:** Work on a copy of this notebook and save your work as indicated at the beginning of the `session_1.ipynb` notebook.\n",
    "\n",
    "As usual, let's first import the functions `start_server_and_interface` and `stop_server_and_interface`, and the `NotebookController` class. Then start the server and interface for this session, create a controller object to interact with the simulator and run it. The code below does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vivarium.controllers.notebook_controller import NotebookController\n",
    "from vivarium.utils.handle_server_interface import start_server_and_interface, stop_server_and_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_server_and_interface(scene_name=\"session_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller = NotebookController()\n",
    "controller.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the interface link shows up (it should be http://localhost:5006/run_interface or similar) and open it in a new browser window. Keep both this notebook and the interface windows open, ideally side by side.\n",
    "In the interface, you will see the scene we will use in this session. It contains three agents (the blue circles), and three different types of objects (green, orange and red squares). Each of these four types of entities is identified by a subtype name that you can access with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.print_subtypes_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`s_obstacles` stands for small obstacles (orange squares), `b_obstacles` for big obstacles (red squares) and `resources` are the green squares. `agents` are the blue circle agents. Those labels can be used to selectively sense each of these entity subtypes, as we have seen in session 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start with a clean basis, let's first provide the definition of several behaviors. These four behaviors are simply implementations of the [Braitenberg vehicles](https://docs.google.com/presentation/d/1s6ibk_ACiJb9CERJ_8L_b4KFu9d04ZG_htUbb_YSYT4/edit#slide=id.g31e1b425a3_0_0) we have seen in class, where the `sensed_entities` (i.e. what is sensed by the proximeters) are other `agents`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fear(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_wheel = left\n",
    "    right_wheel = right\n",
    "    return left_wheel, right_wheel\n",
    "\n",
    "def aggression(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_wheel = right\n",
    "    right_wheel = left\n",
    "    return left_wheel, right_wheel\n",
    "\n",
    "def love(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_wheel = 1 - left\n",
    "    right_wheel = 1 - right   \n",
    "    return left_wheel, right_wheel\n",
    "\n",
    "def shyness(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "    left_wheel = 1 - right\n",
    "    right_wheel = 1 - left   \n",
    "    return left_wheel, right_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a fifth behavior for obstacle avoidance, where obstacles are orange and red squares, labeled `s_obstacles` and `b_obstacles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obstacle_avoidance(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"s_obstacles\", \"b_obstacles\"])\n",
    "    left_wheel = 1 - right\n",
    "    right_wheel = 1 - left   \n",
    "    return left_wheel, right_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing agents interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now analyze the interaction between two agents that are equipped with different behaviors. First assign a variable to each agent of the simulation so we can easily access them indivually in the next steps. As we saw in previous sessions, the `controller.agents` is a list, and we can access its elements with Python indexing system (0 corresponding to the first element, 1 to the second etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0 = controller.agents[0]\n",
    "agent_1 = controller.agents[1]\n",
    "agent_2 = controller.agents[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To differentiate the agents, we will also assign them different colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0.color = \"blue\"\n",
    "agent_1.color = \"cyan\"\n",
    "agent_2.color = \"black\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** In addition to the `obstacle_avoidance` behavior, run several combinations of the four other behaviors defined above. For example, run `obstacle_avoidance` and `fear` on `agent_0` ; and `obstacle_avoidance` and `aggression` on `agent_1`. Keep the `agent_2` still for the moment. Find two of these combinations that you consider as interesting (e.g. because they result in a relatively complex interaction pattern, or because they can be linked to standard animal behavior, e.g. prey-predator) and describe them in a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session's environment contains more entities than previous ones, which might slow down the simulation. In case it is very slow on your computer, you can increase the number of steps the simulation performs on the server for each step done by the controller. To do this, open the `SIMULATOR` panel in the web interface and find the `num_steps_lax` parameter. It is set to 6 by default; you can e.g. double it to 12 to see if the simulation runs faster. Make sure to use this mechanism wisely, as increasing this number too much might lead to a simulation that is less reactive to your commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the previous sesssion and in the question above, it is possible to run several behaviors in parallel on the same agent. When doing it, the motor activation sent to each wheel corresponds to the average of the motor activation returned by each behavior (this averaging is implemented internally, you don't need to worry about it). \n",
    "\n",
    "It is also possible to specify the weight of each running behavior, i.e. how much it will count in the averaging. This is done by passing an optional `weight` argument to the `attach_behavior` method. For example, if we want to run the `obstacle_avoidance` behavior with a weight of 1 and the `fear` behavior with a weight of 0.5 on `agent_2`, we write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2.attach_behavior(obstacle_avoidance, weight=1)\n",
    "agent_2.attach_behavior(fear, weight=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be hard to effectively see the weights in action for these two behaviors as they both lead the agent to avoid other entities. In order to better visualize the effect of the weights, we will define two new behaviors with opposite effects: `aggress_all` and `avoid_all`. The `aggress_all` behavior will make the agent move towards all other entities, while the `avoid_all` behavior will make it move away from them. We will add a bigger weight to the `aggress_all` behavior than to the `avoid_all` behavior, and observe the agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggress_all(agent):\n",
    "    left, right = agent.sensors()  # This senses all entities\n",
    "    left_wheel = right\n",
    "    right_wheel = left  \n",
    "    return left_wheel, right_wheel\n",
    "\n",
    "def avoid_all(agent):\n",
    "    left, right = agent.sensors()  # This senses all entities\n",
    "    left_wheel = 1 - right\n",
    "    right_wheel = 1 - left   \n",
    "    return left_wheel, right_wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_2.detach_all_behaviors()\n",
    "agent_2.attach_behavior(aggress_all, weight=1)\n",
    "agent_2.attach_behavior(avoid_all, weight=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** What are the effects of the weights on the agent's behavior? Why does it keep moving even when it doesn't sense anything ? Describe the observed behavior in a few lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, the wheel activations returned by the `agress_all` behavior will have more weight than those returned by the `avoid_all` behavior. For example, if `avoid_all` returns 0.6 for the left wheel, and `agress_all` returns 0.9, then the total activation of that wheel will be $(0.6 * 0.2 + 0.9 * 1) / (0.2 + 1) = 0.85$ (i.e. the average of both values weighted by their respective activation). Note that when the `weight` argument is not provided to the `attach_behavior` method, the corresponding behavior is set with a default weight of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting behaviors according to internal states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weighting is particularly useful to activate a behavior according to some internal states of the agent. Let's consider the following scenario:\n",
    "\n",
    "- Agents can eat resources (as in the previous session)\n",
    "- Agents have a simulated \"energy level\", which decreases with time and increases whenever they consume a resource.\n",
    "- Agents execute a `foraging` behavior towards resources, with a weight depending on their current energy level: The lower the energy level, the higher the weight.\n",
    "\n",
    "In the following we will implement this scenario step by step.\n",
    "\n",
    "Let's first detach all current behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    agent.detach_all_behaviors(stop_motors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a foraging behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's define a `foraging` behavior (attraction towards resources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foraging(agent):\n",
    "    left, right = agent.sensors(sensed_entities=[\"resources\"])\n",
    "    left_activation = right\n",
    "    right_activation = left\n",
    "    return left_activation, right_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add `resources` to the diet of agents to enable consuming them, as we did in session 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add resources to the all agents' diet\n",
    "for agent in controller.agents:\n",
    "    agent.diet = [\"resources\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way the agent will \"eat\" a resource whenever it is close to it (it will be effective only when we will have activated the eating mechanism below). \n",
    "Do not attach the `foraging` behavior yet, we'll do it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowing when an agent has eaten something\n",
    "\n",
    "To implement the scenario proposed above, we need to continuously compute the energy level of an agent according to how much resources it has recently eaten. To do so, we first need a way to know when a agent has eaten for the last time. There are several ways of doing this that can serve in different use cases:\n",
    "\n",
    "- `agent.has_eaten()`: method returning `True` if the agent has eaten since the last call to this function, `False` otherwise (defaults to False the first time it is called).\n",
    "- `agent.time_since_feeding`: attribute indicating the time since the last time an agent has eaten (defaults to infinity when the agent has never eaten).\n",
    "- `agent.has_eaten_since(t)`: method returning if the agent has eaten since time `t`.\n",
    "\n",
    "Let's print the current values for `agent_0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"agent_0.has_eaten(): {agent_0.has_eaten()} ;  agent_0.time_since_feeding: {agent_0.time_since_feeding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see than when agents haven't started eating, the `has_eaten` method returns `False` and the time since feeding is infinite (`inf`). When it will eat something, the time since feeding will be reset to 0, and the `has_eaten` method will return True. Note that currently agents can't eat anything since we haven't yet activated the eating mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check these values for all agents in the scene at once by iterating through the `controller.agents` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    print(f\"agent_{agent.idx}.has_eaten(): {agent.has_eaten()} ;  agent_{agent.idx}.time_since_feeding: {agent.time_since_feeding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** Remove all current behaviors still running on the agents (if any). Then attach the `obstacle_avoidance` and `foraging` on all agents. Check that they are correctly attached and started with the `print_behaviors` method of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start the eating mechanism and the spawning of resources as in session 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start resources apparition and eating mechanism\n",
    "controller.start_resources_apparition(interval=50)\n",
    "controller.start_eating_mechanism(interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the values of `has_eaten` and `time_since_feeding` evolve for all agents (execute the cell below several time and check that the values change consistently when agents eat resources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    print(f\"agent_{agent.idx}.has_eaten(): {agent.has_eaten()} ;  agent_{agent.idx}.time_since_feeding: {agent.time_since_feeding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using routines to modulate agent's internal states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to continuously compute the energy level of an agent, so that the level decreases slowly when nothing is eaten and increases whenever a resource is consumed. This can be done by attaching to agents what we call a **routine**. The definition of a *routine* is very similar to the definition of a *behavior*: It is defined as a Python function taking an agent as argument. The main difference is that a function defining a routine doesn't return any value (whereas a behavior always returns the left and right wheel activations). Thus, a routine corresponds to a set of instructions that are executed at a particular interval on an agent (similarly to a behavior), e.g. to compute some agent's internal states according to its interaction with the environment.\n",
    "\n",
    "Let's define a routine called `energy` that computes an energy level as specified above. In order to do so, we will first define a few parameters, such as the initial and the maximum level of energy of agents. Then, we will set an initial `energy_level` attribute to each agent:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_energy_level = 1.\n",
    "init_energy_level = 0.5\n",
    "\n",
    "for agent in controller.agents:\n",
    "    agent.energy_level = init_energy_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we can access this attribute for all agents in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    print(f\"agent_{agent.idx} energy level is: {agent.energy_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you execute the cell above several times, note that the `energy_level` attribute of the agents remains the same because we didn't do anything to update it yet. Let's change this by defining the `energy` routine as follows. As for a behavior, the function defining a routine takes an agent as an argument, representing the agent on which the routine will be attached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly to a behavior, a routine is defined as a function that takes an agent as argument\n",
    "# The only difference is that a routine does not return anything (whereas a behavior returns the left and right wheel activations)\n",
    "def energy(agent): \n",
    "    if agent.has_eaten():\n",
    "        # if the agent has eaten a resource since the last execution of the routine, increase its energy level\n",
    "        agent.energy_level += 0.5  # This is equivalent to agent.energy_level = agent.energy_level + 0.5\n",
    "    else:\n",
    "        # decrease energy level\n",
    "        agent.energy_level -= 0.01  # otherwise (nothing eaten), decrease the energy level a bit\n",
    "    # The line below bounds the value of the energy level between 0 and max_energy_level\n",
    "    agent.energy_level = min(max_energy_level, max(agent.energy_level, 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for a behavior, we then need to attach the routine to an agent, which is done with the `attach_routine` method. When attaching a routine, we can also pass an optional `interval` argument to specify every each step it will be executed (defaults to 1). Let's attach the `energy` routine to `agent_0` such that it is executed every 10 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the energy routine to agent_0, which will be executed every 10 steps\n",
    "agent_0.attach_routine(energy, interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before attaching a routine where we update the attribute of an agent, we need to make sure this attribute has been previously initialized. Otherwise the routine will raise an error because it will attempt to access an agent attribute that does not exist. This is why we previously defined the `energy_level` attribute a few cells above (setting it to the initial energy value) before attaching the routine. Whenever you write a routine, you need to make sure that all attributes accessed in it have been previously initialized. \n",
    "\n",
    "\n",
    "\n",
    "Let's check that the energy level of `agent_0` changes according to time and the consumption of resources by executing several time the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_0.energy_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** How does the value change when the agent eats a resource ? What happens when it doesn't eat for a while ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting a behavior according to an agent's internal state\n",
    "\n",
    "To complete the implementation of our scenario, we finally need to weight the `foraging` behavior according to energy level of the agent. The lower the energy level, the higher `foraging` is weighted. We have seen above how we can weight a behavior when attaching it. We can also dynamically change it (i.e. while the behavior is running on the agent) with the `change_behavior_weight` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the weight of the foraging behavior on agent_0 to 0.5:\n",
    "agent_0.change_behavior_weight(foraging, new_weight=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different values of the `new_weight` argument above and check that the `agent_0` (the dark blue one) behaves as expected. In particular, if the weight is set to 0, you should observe that agent no longer forage for resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want this weight to be continuously updated according the current energy level of an agent, we can define another routine for doing this. Let's call this new routine `foraging_weight`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foraging_weight(agent):\n",
    "    # This routine changes the weight of the foraging behavior according to the current energy level\n",
    "    # The lower the energy level, the higher the weight (energy level is bounded between 0 and 1 in the energy routine)\n",
    "    # E.g., if the energy is 1 (maximum value), the behavior weight will be 0 (and vice versa)\n",
    "    agent.change_behavior_weight(foraging, 1 - agent.energy_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attach the `foraging_weight` routine to `agent_0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach the foraging_weight routine to agent_0, which will be executed every 10 steps\n",
    "agent_0.attach_routine(foraging_weight, interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the weight of the `foraging` behavior of `agent_0` is modulated by its energy level, as implemented in the `foraging_weight` routine above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the current weights of all attached behaviors with the `print_behaviors` method by setting the `full_infos` argument to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_0.print_behaviors(full_infos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can re-execute the cell above several times to check that the weight of the `foraging` behavior of `agent_0` is modulated according to the energy level as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can detach the routine from an agent similarly to a behavior with the `detach_routine` and `detach_all_routines` functions, and check them with the `print_routines` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only detach the foraging_weight routine\n",
    "agent_0.detach_routine(foraging_weight)\n",
    "\n",
    "# Detach all currently attached routines\n",
    "agent_0.detach_all_routines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you re-execute the `print_behaviors` method above several times, you should observe that the weight of the `foraging_behavior` is no longer modulated by the energy level (i.e. the weight remains constant at its last value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it up\n",
    "We now have all the ingredients for our scenario. Let's wrap it up by detaching all attached behaviors and routines, attaching both the `obstacle_avoidance` and `foraging` behaviors as well as both the `energy` and `foraging_weight` routines, to all agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents: # For all agents\n",
    "    agent.detach_all_behaviors(stop_motors=True)  #Detach all previously attached behaviors\n",
    "    agent.detach_all_routines()  #Detach all previously attached routines\n",
    "    agent.attach_behavior(foraging)  # Attach the foraging behavior\n",
    "    agent.attach_behavior(obstacle_avoidance)  # Attach the obstacle avoidance behavior\n",
    "    agent.attach_routine(energy, interval=10)  # Attach the energy routine\n",
    "    agent.attach_routine(foraging_weight, interval=10)  # Attach the foraging_weight routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `foraging_weight` routine has to be attached **after** the `foraging` behavior, because this routine changes the weight of the behavior. If the routine is attached before the behavior, it might raise an error because it will try to access a behavior that is not yet attached.\n",
    "\n",
    "In the interface, you should observe that all agents are indeed avoiding obstacles (red and orange squares), are attracted toward resources and consume them. When they eat a few resources in a row, their energy level approaches the maximum value. In this situation, you should observe that the agents are no longer attracted toward resources, until their energy level decreases again. You can check the corresponding values by printing the energy level and the behavior weights of all agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    print(f'agent_{agent.idx}')\n",
    "    print('------------')\n",
    "    print(f'energy level: {agent.energy_level}')\n",
    "    agent.print_behaviors(full_infos=True)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Detach the `energy` routine on all agents (only this routine, not the `foraging_weight` one) and set the energy level of all agents to 1 (i.e. the maximum value). Describe what happens in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now detach all the behaviors and the routines before going to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    agent.detach_all_routines()\n",
    "    agent.detach_all_behaviors(stop_motors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensing entity's attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to allow an agent to sense attributes of other entities. As a reminder, we call \"entity\" any item present on the scene, either agents or objects (as explained in the [web interface tutorial](../tutorials/web_interface_tutorial.md)). For example, one might want to define different species of agents (cats and mouses for example) such that how an agent interacts with another depends on their respective species. To achieve this we can define arbitrary attributes of agents, e.g `agent.species = \"cat\"` and that attribute can be \"sensed\" by other agents whenever it is detected by a proximeter sensor. Let's try it with three agents. \n",
    "\n",
    "We define a `species` atttribute for all agents (`species` is an arbitrary attribute name, it could be anything else). The species of `agent_0` is `\"cat\"` and we make it bigger, and the species of `agent_0` and `agent_1` is  `\"mouse\"`  and we maked them smaller. We also  change the color of mouses to distinguish them from the cat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set attributes of agent 0\n",
    "agent_0.species = \"cat\"\n",
    "agent_0.diameter = 12.\n",
    "\n",
    "# set the same attributes for agent 1 and agent 2, and change their color\n",
    "agent_1.species = agent_2.species = \"mouse\"\n",
    "agent_1.diameter = agent_2.diameter = 7.\n",
    "agent_1.color = agent_2.color = \"cyan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have one cat and two mouses. Place the agents (either with the interface or with code) so that `agent_2` (mouse) is sensing `agent_0` (cat) on its left proximeter and `agent_1` (mouse) on its right proximeter. You can set the position of an entity by using its `x_position` and `y_position` attributes. For instance you can do it with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import pi  # import the number pi, we'll use it to set the agent orientation\n",
    "\n",
    "# First place the agent 2 at a specific location, and make it face the top of the environment\n",
    "agent_2.x_position = 150\n",
    "agent_2.y_position = 150\n",
    "agent_2.orientation = pi / 2  # Orientations are specified in radians, with 0 corresponding to the left direction. He we set it to pi / 2, making the agent face the top direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, place the two other agents accordingly so they are sensed on the left and right of agent 2\n",
    "agent_0.x_position = 130\n",
    "agent_0.y_position = 170\n",
    "\n",
    "agent_1.x_position = 170\n",
    "agent_1.y_position = 170"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just make sure there are no objects sensed between `agent_2` (at the bottom) and the two other agents. You can manually move the objects in the interface if it is the case. The center of your scene should look roughly like this image:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Then, to return the entities sensed by the left and right proximeters of `agent_2`, we can use the `entity_sensors` method of the agents. This returns a reference to the closest left and right entities within the agent field of view, if any, and `None` otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_left, ent_right = agent_2.entity_sensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what are the entities sensed by an agent by running the following cell. Let's analyze it with more details. First, we print infos for the entity sensed by the left sensor. We start by ensuring it exists with `if ent_left:`, because it is possible that no entity is sensed and this value will therefore be `None`. Then if the entity exists, we print its infos with the `print_infos()` method, else we just print that there are no entity sensed on this side. We do the same on the right side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the infos of the sensed entities\n",
    "print(\"Entity sensed by the left sensor:\")\n",
    "if ent_left:\n",
    "    ent_left.print_infos()\n",
    "else:\n",
    "    print(\"No entity sensed\")\n",
    "\n",
    "print(\"Entity sensed by the right sensor:\")\n",
    "if ent_right:\n",
    "    ent_right.print_infos()\n",
    "\n",
    "else:\n",
    "    print(\"No entity sensed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the three agents were placed as specified above, the left entity should be the `agent_O` (Idx: 0) and the right entity should be `agent_1` (Idx: 1). Note that the `ent_left` and `ent_right` entities are the exact same as the `agent_0` and `agent_1` ones, respectively. In Python, we can check they are the same using the `==` sign: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_left == agent_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cell above returns `True`, it means that the left entity returned by the `agent_2.entity_sensors()` method is (literally) `agent_0`. We can therefore manipulate the `ent_left` variable the exact same way we would manipulate any agent variable. Let's for example access its diameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ent_left.diameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and its species (that was defined above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ent_left.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly sense specific attributes of the sensed entities in a single line of code with the `attribute_sensors` method. The method expects a `sensed_attribute` argument, indicating which attribute we want the agent to sense (e.g 'diameter', 'species', etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's directly access the `diameter` attribute of the entities sensed by `agent_2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_attr, r_attr = agent_2.attribute_sensors(sensed_attribute=\"diameter\")\n",
    "print(f\"Diameter of the entity sensed by the left sensor: {l_attr}\")\n",
    "print(f\"Diameter of the entity sensed by the right sensor: {r_attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you correctly positioned the three agents as specified above, it should return two numbers indicating the diameter of the agents sensed by the left and right sensors of `agent_2`. Otherwise, it is possible that the value of one sensor returns `None`: in that case, it means that either no entity was sensed, or that the attribute does not exist on the sensed entity. We can return another value than `None` in that case by specifying a `default_value` argument. The `attribute_sensors` method will return the given attribute of the left and right sensed entities if it exists, and the default value otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's access the `species` attribute of the sensed entities, setting `default_value` to the string `\"unknown\"`. If no entity is detected by the sensor, or if the sensed entity has no `species` attribute (it will be e.g. the case if the sensed entity is an object), the method will return 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_attr, r_attr = agent_2.attribute_sensors(sensed_attribute=\"species\", default_value=\"unknown\")\n",
    "print(f\"Left: {l_attr} Right: {r_attr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these two methods, `entity_sensors` and `attribute_sensors`, work with any type of sensed entities, either agent or objects.\n",
    "\n",
    "**Q6:** Define an `age` attribute on all agents and write the code to sense it. Then write the code to sense the `orientation` attribute of any entity (the `orientation` attribute already exist on both agents and objects, no need to define it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By moving agents in the scene (either on the web interface of with code), check that an agent can indeed sense the `age` attribute of other agents, but that the `default_value` is returned when it instead senses an object. Also check that to `orientation` attribute can be sensed on any entity, either agents or objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods explained above enable to implement more complex scenarios. Let's for example constrain the `fear` behavior so that mouses are only afraid by the cat but not by the other mouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fear_cat(agent):\n",
    "    # Access the proximeter values as we did in previous sessions, selectively sensing other agents\n",
    "    left, right = agent.sensors(sensed_entities=[\"agents\"])\n",
    "\n",
    "    # Access the species attribute of the sensed entities\n",
    "    left_species, right_species = agent.attribute_sensors(sensed_attribute=\"species\")\n",
    "    \n",
    "    # Compute wheel activation depending on whether a cat was sensed or not\n",
    "    left_activation = left if left_species == \"cat\" else 0  # set the left wheel to the left proximeter value (i.e. fear behavior) if the sensed entity is a cat, 0 otherwise\n",
    "    right_activation = right if right_species == \"cat\" else 0  # Idem on the right side\n",
    "    \n",
    "    # Return wheel activations as usual\n",
    "    return left_activation, right_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the obstacle avoidance on all agents to make them move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    agent.attach_behavior(obstacle_avoidance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also attach the `fear_cat` behavior on the two mouses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in controller.agents:\n",
    "    if agent.species == \"mouse\":  # Only attach the fear_cat behavior to mouse \n",
    "        agent.attach_behavior(fear_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check in the simulator that the two mouses (small cyan agents) are now avoiding the cat (bigger dark blue agent) but not the other mouse.\n",
    "\n",
    "We can also modify the `species` attribute of agents on the fly. Let's swap the species of `agent_0` and `agent_1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change agent_O to a green mouse\n",
    "agent_0.species = \"mouse\"\n",
    "agent_0.diameter = 7.\n",
    "agent_0.color = \"green\"\n",
    "\n",
    "# Change agent_1 to a blue cat\n",
    "agent_1.species = \"cat\"\n",
    "agent_1.diameter = 12.\n",
    "agent_1.color = \"blue\"\n",
    "\n",
    "# Since agent_1 is now a cat, we can detach the fear_cat behavior from it\n",
    "agent_1.detach_behavior(fear_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check in the simulator that `agent_2` (cyan mouse) is now avoiding `agent_1` (the new cat) but not `agent_0` (the new green mouse). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Use the new functionalities we have seen in this session to design the following scenario:\n",
    "The three agents are equipped with behaviors to avoid obstacles and forage for resources, as well as being attracted or repulsed by other agents (i.e. 4 behaviors in total). Attraction and repulsion depend on the energy level of the sensed agent: the higher this level the more attraction, the lower this level the more repulsion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first detach all the potential behaviors and routines currently attached\n",
    "for agent in controller.agents:\n",
    "    agent.detach_all_behaviors(stop_motors=True)\n",
    "    agent.detach_all_routines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this session. As usual, don't forget to properly close it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_server_and_interface(safe_mode=False)\n",
    "controller.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you finished this session, you can jump to :\n",
    "\n",
    "- [session 5](session_5_logging.ipynb) : How to log and plot data about the simulation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
